# Transformer-breakdown

The goal of this project is to learn transformer by explaining everything in a simple way and coding them from scratch, so even people without a strong math background can understand. This follows the same spirit as the other Zero2ML projects.

Note: This project will not provide a full encoder-decoder transformer implementation. Instead, we will focus on building a simplified demo version of a transformer with the **attention** mechanism to clearly illustrate the main concepts. The goal is to help you understand the core ideas.

## Prerequisite

For this project, some basic machine learning prerequisites will be needed. I recommend reviewing the [MultiLayer Perceptron (MLP)](https://github.com/swangarch/multilayer_perceptron) chapter first.

Here are some key prerequisites:

- [**Matrix Multiplication**](https://en.wikipedia.org/wiki/Matrix_multiplication): Fundamental operation in linear algebra used in neural networks.

- [**Neural Network**](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)): Computational model inspired by the human brain, used for pattern recognition.

- [**Feed forward**](https://en.wikipedia.org/wiki/Feedforward_neural_network): A step of neural network where information moves forward from input to output.

- [**Back propagation**](https://en.wikipedia.org/wiki/Backpropagation): Algorithm used to compute gradients for weight updates using chain rule.

- [**Gradient Descent**](https://en.wikipedia.org/wiki/Gradient_descent): Optimization algorithm used to minimize the loss function of a neural network.

- [**Softmax**](https://en.wikipedia.org/wiki/Softmax_function): Function that converts a vector of values into a probability distribution.

All other concepts required for understanding transformers will be explained along with the project step-by-step.

## Transformer

The Transformer is the main architecture behind all modern large models. It was introduced in the famous paper **Attention is All You Need**. Then it was quickly adopted by BERT and GPT, and later became the foundation of almost all large models‚Äînot only LLMs, but also image models, audio models, video models, and multimodal models. Thanks to their contributions, AI has made remarkable progress and had a huge impact.

Before writing any code, let‚Äôs explain things visually. It‚Äôs important to build intuition before touching abstract formulas.

## Language representation

To make human language understandable to machines, we first need a numerical representation. We do this using **embeddings**.

Let‚Äôs take a simplified example: we represent each word with a 3-dimensional vector, so a sentence of 4 words becomes a 4√ó3 matrix.

![Representation](assets/svg/T1.svg)

Word **order** is essential for **meaning**. The probability of a word appearing at a certain position depends heavily on its positional relationship with other words.
To **understand** language, the model must capture these relationships.

Unlike a time-sequence model (like RNNs), which can forget long-range dependencies, Transformers use **attention**, which lets every word look at every other word in the sentence.

## Attention 

So, what is attention?


$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$

This formula looks strange at first‚ÄîQ, K, and V seem to appear out of nowhere‚Äîbut it‚Äôs actually simple and elegant once you understand it. The only prerequisite is knowing basic matrix multiplication.

### Q K V
Q, K, and V are matrices generated by multiplying the input with weight matrices ùëäq, ùëäk, and ùëäv. These weights are **learnable** parameters of the Transformer. They represent different **views** of the same input.

Each matrix corresponds to a simple idea:

- Q (Query) : What am i looking for? 
- K (Key) : Where can i find it?
- V (Value): What is the actual content?

![Attention](assets/svg/T2.svg)

Each column of Q and each row of ùêæt(K transpose) corresponds to a word. When we multiply Q with ùêæt, we are basically computing dot products word-by-word. The dot product measures **similarity**, so the result is a matrix showing how much each word relates to every other word.

Although we call the dot product **similarity**, here it **doesn‚Äôt** mean two words are **semantically similar** in the real world. Instead, it represents how much the Query of one word matches the Key of another word in the **current context**.

For example, in the sentence: I'm playing with a cat.

Here, **I** is not similar to **cat**, but I mostly focus on the cat in the current context. In other words, this similarity is actually a measure of relevance‚Äîhow useful one word is to another at this moment. This relevance score is what later becomes **attention weights** after we apply softmax.

The division by square root of ùëëùëò just prevents the dot products from becoming too large.

![Attention](assets/svg/T3.svg)

Then **softmax** converts each row of similarities into probabilities between 0 and 1, and the sum is 1.

![Attention](assets/svg/T4.svg)


Here you may have the doubt: **Why**? 
Why do these Q and K scores become attention weights? Why can they represent how much one word should attend to another?

The key point is:
**Q and K are not born as attention** ‚Äî training forces them to become attention. At the beginning, Q and K are just random linear projections. Their dot products are meaningless. But during training, the model must reduce the **loss** (for example, try to correctly predicting the next token). To do that, it has to figure out which words are useful and which words it should ignore.

So the learning process, through **feed forward**, **back propagation**, and **gradient descent**, pushes Q to represent what this word needs, and pushes K to represent what this word offers. The dot product between them gradually becomes a measure of relevance in the current context, because that is the most effective structure for **minimizing error**.

After softmax, these learned relevance scores become the attention weights ‚Äî telling the model how much each word should look at every other word.

---
Next, we use this probability matrix to multiply the V matrix.
The result is another matrix with the same size as Q, K, and V, which represents how much each word attends to the others, to get the attention value.
We call this process so far an **attention head**.

$head_i = Attention(Q W^Q_i, K W^K_i, V W^V_i)$

![Attention](assets/svg/T5.svg)


### Multi-head attention

One attention head gives us one perspective. But we can repeat this process multiple times to look at the data from different perspectives. We then concatenate all heads and project them back to the input dimension through a linear projection, by multiplying by a learnable weight matrix $W^O$:

$MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O$


This gives us a matrix with the same shape as the original input, but now enriched with multi-perspective attention information.

![Attention](assets/svg/T6.svg)

That‚Äôs it for the attention mechanism.

<!-- ## On going
...

![Attention](assets/svg/T7.svg) -->