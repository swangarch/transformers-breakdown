# Transformers-breakdown

The goal of this project is learn transformer by explaining, by coding from scrath, to make it understandble even for people who don't have strong foundation in math like myself. With the same spirit as the other projects in the series of Zero2ML.

## Transformers

Transformer is the main model architecture that powers all the large model we used today, it was introduced in the famous paper ***Attention is all you need***. It was then quickly adopted by Bert and GPT, then become the foundation of all the large model, not only LLM, but also images model, audios model, videos model, and multi modalities model.

Before writing any code, let's just explain, and draw diagrams, it's important to have a visual intuition before going to abstract formulars.

## Language representation

To make the human language understanderble for machine, we need first to find a numerical representation of language, the way we used is called embedding.

![Representation](assets/svg/T1.svg)

Let's take a simplified case, we use 3 dimensional vector to represente 1 word, so we have a matrix of 4 X 3 which describe a sentence.

As the order of language is essential for their meaning, and the probabilities of the existence of a word in a specific position of one sentence, is largely determined by the positional relationship with other words. To usderstand language, or to pretend to understand, the model has to find this relationships between words. Compare to a time sequence model, which could forget the long term relationship, Transformer use a mecanism called attention, which allows each word to observe the relationship between all other words in a context.

## Attention 

Now, what is attention?

$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$

This formular looks weird, especailly, the Q, K, V, comme out of nowhere, but dont be scared it is actually quite simple and beautiful. To understand, the prerequis is Matrix Multiplication, which you can find a lot of ressource to learn it.

### Q K V
Q,K,V are matrixs generated by matmul of corresponding weights matrix WQ, WK, WV. Those weights matrix are parts of the learnable parameters of transformer. It's the way to look at the input matrix with different perspective.

Each Matrix reprensent something simple but essential to the model training.

Q (Query) : What am i looking for? 
K (Key) : Where can i find it?
V (Value) : What is the content?

These 3 matrix reprents 3 different perspective of same input matrix.

![Attention](assets/svg/T2.svg)

Each column of Q and each row of K's transpose is a word.
The operation we do here, Q matmul with K's transpose, if we break down, we just calculate the dot product each column of Q and each row of K's transpose, the product.

![Attention](assets/svg/T3.svg)

![Attention](assets/svg/T4.svg)

![Attention](assets/svg/T5.svg)

$MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O$

$head_i = Attention(Q W^Q_i, K W^K_i, V W^V_i)$

![Attention](assets/svg/T6.svg)

### Residual connection

![Attention](assets/svg/T7.svg)